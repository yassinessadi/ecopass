{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "---\n",
       "**Authors:** Essadi <---> (Jane & Yassine) \n",
       "---\n",
       "\n",
       "---\n",
       "- \\( H \\) : Header (2 bytes)\n",
       "- \\( S \\) : Status (first 4 bits of \\( B_0 \\))\n",
       "- \\( L \\) : Length (remaining 12 bits)\n",
       "- \\( D \\) : Data (record content)\n",
       "- \\( P \\) : Padding (if \\( L \\) is odd)\n",
       "- \\( HS \\) : Header Size (2)\n",
       "- \\( T \\) : Total Header Size (HS + L)\n",
       "- \\( A \\) : Alignment (2)\n",
       "- \\( PS \\) : Padding Size\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle H = [B_0, B_1]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle S = \\frac{B_0}{16}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle L = (B_0 \\mod 16) \\times 256 + B_1$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle D = f.read(L)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle |D| = L$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle P = (L \\mod 2)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{T} = HS + L$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{PS} = (A - (T \\% A)) \\% A$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Markdown\n",
    "\n",
    "legend_text = r\"\"\"\n",
    "---\n",
    "**Authors:** Essadi <---> (Jane & Yassine) \n",
    "---\n",
    "\n",
    "---\n",
    "- \\( H \\) : Header (2 bytes)\n",
    "- \\( S \\) : Status (first 4 bits of \\( B_0 \\))\n",
    "- \\( L \\) : Length (remaining 12 bits)\n",
    "- \\( D \\) : Data (record content)\n",
    "- \\( P \\) : Padding (if \\( L \\) is odd)\n",
    "- \\( HS \\) : Header Size (2)\n",
    "- \\( T \\) : Total Header Size (HS + L)\n",
    "- \\( A \\) : Alignment (2)\n",
    "- \\( PS \\) : Padding Size\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(legend_text))\n",
    "\n",
    "\n",
    "display(Math(r\"H = [B_0, B_1]\"))                       # Read 2-byte header\n",
    "display(Math(r\"S = \\frac{B_0}{16}\"))                   # Extract status\n",
    "display(Math(r\"L = (B_0 \\mod 16) \\times 256 + B_1\"))   # Extract length\n",
    "display(Math(r\"D = f.read(L)\"))                        # Read record data\n",
    "display(Math(r\"|D| = L\"))                              # Check completeness\n",
    "display(Math(r\"P = (L \\mod 2)\"))                       # Handle padding\n",
    "display(Markdown(r\"\"\"---\"\"\"))\n",
    "\n",
    "\n",
    "# Equations for total header data and padding size\n",
    "display(Math(r\"\\text{T} = HS + L\"))\n",
    "display(Math(r\"\\text{PS} = (A - (T \\% A)) \\% A\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Step</th>\n",
    "        <th>Explanation</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Read 2-byte header</td>\n",
    "        <td>Read two bytes from the binary file, storing them as \\( B_0 \\) and \\( B_1 \\).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Extract status</td>\n",
    "        <td>Take the first 4 bits of \\( B_0 \\) by performing a right shift of 4 bits: \\( S = (B_0 >> 4) \\).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Extract length</td>\n",
    "        <td>Use the last 4 bits of \\( B_0 \\) and combine with \\( B_1 \\) to get a 12-bit length value.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Read record data</td>\n",
    "        <td>Read \\( L \\) bytes from the file as the record data.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Check completeness</td>\n",
    "        <td>Ensure that the number of bytes read matches the expected length \\( L \\); otherwise, raise an error.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Handle padding</td>\n",
    "        <td>If the length \\( L \\) is odd, read 1 additional byte as padding.</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ **Initial Position**  \n",
    "When the file is opened with `open(filename, 'rb')`, the file pointer is at the **beginning (byte 0)**.\n",
    "\n",
    "### 2Ô∏è‚É£ **Reading the Header (2 bytes)**  \n",
    "- The first iteration reads **2 bytes** (`header = f.read(2)`).  \n",
    "- The file pointer moves forward by **2 bytes**.\n",
    "\n",
    "### 3Ô∏è‚É£ **Extracting `status` and `length`**  \n",
    "- The **status** is extracted from the first 4 bits of `byte0`.  \n",
    "- The **length** (L) of the record is determined from the remaining 12 bits.\n",
    "\n",
    "### 4Ô∏è‚É£ **Reading the Record Data (`L` bytes)**  \n",
    "- The function reads **L bytes** (`data = f.read(length)`).  \n",
    "- The file pointer moves forward by **L bytes**.\n",
    "\n",
    "### 5Ô∏è‚É£ **Handling Padding** (if `L` is odd)  \n",
    "- If `L` is **odd**, one extra byte (padding) is read (`f.read(1)`).  \n",
    "- This ensures alignment for the next record.\n",
    "\n",
    "### üîÑ **Next Iteration (New Position)**  \n",
    "- The next iteration of the `while` loop starts from **where the last read ended** because the file pointer was never reset.  \n",
    "- The pointer moves forward **by (2 + L + padding) bytes** per record.\n",
    "\n",
    "### ‚úÖ **Key Concept**  \n",
    "The file pointer **does not reset** after reading because `f.read(N)` **advances the pointer** automatically. Each iteration picks up where the previous one left off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Dev Box Centered Image</title>\n",
    "    <style>\n",
    "        * {\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            box-sizing: border-box;\n",
    "        }\n",
    "        .dev-box {\n",
    "            display: flex;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "            border: 2px solid #00ff00;\n",
    "            box-shadow: 0 0 10px #00ff00;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dev-box\">\n",
    "        <img src=\"./assets/bitwise-right-shift-operator.png\" height=\"300\" />\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='color:orange' width=280/>\n",
    "<hr style='color:orange' width=380/>\n",
    "<hr style='color:orange' width=480/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max rdd size for spark: (2^31 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='color:skyblue'/>\n",
    "<hr style='color:skyblue'/>\n",
    "<hr style='color:skyblue'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, BinaryType, FloatType,StringType\n",
    "import schema.ENR_TFONC as sc\n",
    "from array import array\n",
    "import enum\n",
    "import mmap\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ======================================================================\n",
    "### 1. Spark Session Configuration\n",
    "### ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LargeScaleBinaryProcessor\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.default.parallelism\", \"100\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "### ======================================================================\n",
    "# If running in a Jupyter Notebook, you can import clear_output:\n",
    "### ======================================================================\n",
    "\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "except ImportError:\n",
    "    clear_output = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ======================================================================\n",
    "### 2. Distributed Binary File Processing\n",
    "### ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordType(enum.Enum):\n",
    "    SYSTEM_RECORD = 1\n",
    "    DELETED_RECORD = 2\n",
    "    SYSTEM_RESERVED = 3\n",
    "    NORMAL_USER_DATA = 4\n",
    "    POINTER_RECORD = 6\n",
    "    USER_DATA_RECORD = 7\n",
    "    RESERVED_FOR_FUTURE_USE = 9\n",
    "\n",
    "class RecordHeader():\n",
    "    def __init__(self, filename, record_length: list = [], header_size: int = 2, alignment: int = 2):\n",
    "        if header_size not in {2, 4}:\n",
    "            raise ValueError(\"Only 2-byte or 4-byte headers are supported\")\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.header_size = header_size\n",
    "        self.record_length = record_length\n",
    "        self.allowed_statuses = [\n",
    "            RecordType.NORMAL_USER_DATA.value,\n",
    "            RecordType.SYSTEM_RESERVED.value,\n",
    "            RecordType.USER_DATA_RECORD.value,\n",
    "            RecordType.RESERVED_FOR_FUTURE_USE.value,\n",
    "            RecordType.SYSTEM_RECORD.value,\n",
    "            RecordType.POINTER_RECORD.value,\n",
    "        ]\n",
    "        self.alignment = alignment\n",
    "        self.file = None        # Will be set in __enter__\n",
    "        self.mmap_obj = None    # Will be set in __enter__\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Opens the file and creates a memory map for it.\"\"\"\n",
    "        self.file = open(self.filename, 'rb')\n",
    "        self.mmap_obj = mmap.mmap(self.file.fileno(), length=0, access=mmap.ACCESS_READ)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"Closes the memory map and file when exiting the context.\"\"\"\n",
    "        if self.mmap_obj:\n",
    "            self.mmap_obj.close()\n",
    "        if self.file:\n",
    "            self.file.close()\n",
    "\n",
    "    def _parse_header(self, header_bytes: bytes) -> tuple[int, int]:\n",
    "        if self.header_size == 2:\n",
    "            byte0, byte1 = header_bytes\n",
    "            # Extract status (first 4 bits of byte0)\n",
    "            status = (byte0 >> 4) & 0x0F\n",
    "            # Extract length (remaining 12 bits)\n",
    "            length = ((byte0 & 0x0F) << 8) | byte1\n",
    "        elif self.header_size == 4:\n",
    "            # Extract status (first 4 bits of byte0)\n",
    "            byte0, byte1, byte2, byte3 = header_bytes\n",
    "            status = (byte0 >> 4) & 0x0F\n",
    "            # Extract length (remaining 12 bits)\n",
    "            length = ((byte0 & 0x0F) << 24) | (byte1 << 16) | (byte2 << 8) | byte3\n",
    "\n",
    "        return status, length\n",
    "    \n",
    "    def _parse_padding(self, length: int):\n",
    "        # --------------------------------------------\n",
    "        # Example :\n",
    "        # (2 - (107 + 2 % 2)) % 2\n",
    "        # --------------------------------------------\n",
    "        # if length % 2 != 0:\n",
    "        #     f.seek(1, 1)\n",
    "\n",
    "        total_header_data = self.header_size + length\n",
    "        padding_size = (self.alignment - (total_header_data % self.alignment)) % self.alignment\n",
    "        return padding_size\n",
    "\n",
    "\n",
    "    def read_records(self):\n",
    "        try:\n",
    "            f = self.mmap_obj\n",
    "            while True:\n",
    "                # Read the 2-byte header\n",
    "                pos = f.tell()\n",
    "                header = f.read(self.header_size)\n",
    "                if len(header) < self.header_size:\n",
    "                    break  # End of file or incomplete header\n",
    "                \n",
    "                status, length = self._parse_header(header)\n",
    "\n",
    "                # Read the record data\n",
    "                data = f.read(length)\n",
    "                \n",
    "                if len(data) < length:\n",
    "                    raise ValueError(f\"Incomplete data at position {pos}\")\n",
    "\n",
    "                # if status in self.allowed_statuses and length in self.record_length:\n",
    "                if status in self.allowed_statuses:\n",
    "                    yield length, bytes(data)\n",
    "\n",
    "                padding_size = self._parse_padding(length=length)\n",
    "\n",
    "                f.seek(padding_size, 1)\n",
    "        except:\n",
    "            raise ValueError(f\"Incomplete data at position {pos}\")\n",
    "        finally:\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='color:orange'>\n",
    "<hr style='color:orange'>\n",
    "<hr style='color:orange'>\n",
    "<hr style='color:orange'>\n",
    "\n",
    "To optimize writing **1 M records per batch**, we need to balance **memory efficiency**, **parallelism**, and **disk I/O performance**. Here's an optimized approach we need to follow:\n",
    "\n",
    "### **Optimized Batch Writing Strategy**\n",
    "1. **Batch Processing (1M records per batch)**\n",
    "2. **Parallelizing Writes (`numSlices=7`)**\n",
    "3. **Explicit Caching & Repartitioning (`coalesce(4)` to reduce small files)**\n",
    "4. **Tuning `parquet.block.size` for better disk I/O performance**\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This is Optimized**\n",
    "‚úÖ **Memory Efficiency**: Processing in **1M record batches** prevents memory overload.  \n",
    "‚úÖ **Parallelism**: `numSlices=7` ensures efficient **CPU core usage**.  \n",
    "‚úÖ **Parquet Compression (`snappy`)**: Fast compression with minimal CPU overhead.  \n",
    "‚úÖ **Partition Optimization (`repartition(4)`)**: Prevents small file issues.  \n",
    "‚úÖ **Spark Configuration Optimizations**:\n",
    "   - **`spark.sql.files.maxPartitionBytes = 128MB`**: Optimizes partition sizes for large files.\n",
    "   - **`spark.sql.shuffle.partitions = 7`**: Reduces shuffle overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### **Want Even Faster Writes?**\n",
    "üîπ **Increase `numSlices` to match your CPU cores.**  \n",
    "üîπ **Try `coalesce(2)` instead of `repartition(4)` if writing fewer large files is preferable.**  \n",
    "üîπ **Use a larger block size for Parquet (`parquet.block.size=256MB`).**\n",
    "\n",
    "<hr style='color:orange'>\n",
    "<hr style='color:orange'>\n",
    "<hr style='color:orange'>\n",
    "<hr style='color:orange'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Records in Batches\n",
    "\n",
    "This script reads records from a file and processes them in batches to optimize performance and memory usage.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Initialize the Record Reader**  \n",
    "   - The `RecordHeader` context manager is used to open and read the file.\n",
    "   - The `filename` is set to `path_input`, and the records are read with a `header_size` of 2 and `alignment` of 2.\n",
    "\n",
    "2. **Iterate Through Records**  \n",
    "   - The `read_records()` method returns `length` and `data` for each record.\n",
    "   - These records are stored in a list named `records`.\n",
    "\n",
    "3. **Batch Processing**  \n",
    "   - If the number of records in `records` reaches or exceeds `batch_size`, the following steps are executed:\n",
    "     - Convert the list of records into an RDD with `numSlices=7` for parallel processing.\n",
    "     - Create a Spark DataFrame from the RDD using the defined `schema`.\n",
    "     - Reduce the number of output files by coalescing the DataFrame into 7 partitions.\n",
    "     - Write the DataFrame to a Parquet file, partitioning by the `\"rdw\"` column in **append mode**.\n",
    "\n",
    "4. **Memory Management**  \n",
    "   - The `records` list is cleared to free up memory.\n",
    "   - The DataFrame and RDD are explicitly **unpersisted**.\n",
    "   - Spark's catalog cache is cleared.\n",
    "   - The variables `rdd` and `df` are deleted to further manage memory.\n",
    "\n",
    "## Summary:\n",
    "\n",
    "- **Batch processing** ensures efficient writing to Parquet.\n",
    "- **Memory cleanup** prevents excessive memory usage.\n",
    "- **Parallelization** is handled using Spark's RDD and DataFrame APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==================================================================================================================\n",
    "### 3. Create DataFrame from list of records and length of each record using batch logic since the data it's so large\n",
    "### =================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch writing complete.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# User Input: Define File Paths\n",
    "# -------------------------------------------\n",
    "\n",
    "path_intput = \"../data/filename\"\n",
    "path_output = \"../output/filename.parquet\"\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# Define Schema for Spark DataFrame\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"rdw\", IntegerType()),\n",
    "    StructField(\"value\", BinaryType())\n",
    "])\n",
    "\n",
    "# Read Records in Batches\n",
    "# --------------------------------------------------------------\n",
    "# # batch size based on memory constraints -> 1_900_000 * 836 bytes = ~950 MB\n",
    "# --------------------------------------------------------------\n",
    "batch_size = 1_900_000  \n",
    "records = []\n",
    "\n",
    "\n",
    "with RecordHeader(filename=path_intput, header_size=2, alignment=2) as record_header:\n",
    "    for length, data in record_header.read_records():\n",
    "        records.append((length, data))\n",
    "        \n",
    "        # Write to Parquet when batch size is reached\n",
    "        if len(records) >= batch_size:\n",
    "            rdd = spark.sparkContext.parallelize(records, numSlices=7)\n",
    "            df = spark.createDataFrame(rdd, schema=schema)\n",
    "            # Reduce number of output files\n",
    "            df = df.coalesce(7)\n",
    "\n",
    "            # Write in batches to Parquet\n",
    "            df.write.partitionBy(\"rdw\").mode(\"append\").parquet(path_output)\n",
    "\n",
    "            # Clear records to avoid memory issues\n",
    "            records.clear()\n",
    "            df.unpersist(blocking=True)\n",
    "            rdd.unpersist(blocking=True)\n",
    "            spark.catalog.clearCache()\n",
    "            del rdd, df\n",
    "\n",
    "## Write remaining records if any\n",
    "##\n",
    "## since the loop will iterate may some chunks less then 1_900_000 so the code will run last condition if any remaining records here <<J a N e@@yushin>>\n",
    "print(\"Batch writing complete. and start for remaining records if any recors still op there.\")\n",
    "##\n",
    "\n",
    "# -------------------------------------------\n",
    "# Write Remaining Records (if any)\n",
    "# -------------------------------------------\n",
    "if records:\n",
    "    rdd = spark.sparkContext.parallelize(records, numSlices=7)\n",
    "    df = spark.createDataFrame(rdd, schema=schema)\n",
    "    df.write.partitionBy(\"rdw\").mode(\"append\").parquet(path_output)\n",
    "    records.clear()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Cleanup Section\n",
    "# -------------------------------\n",
    "\n",
    "# -------------------------------------------\n",
    "# Cleanup and Garbage Collection\n",
    "# -------------------------------------------\n",
    "gc.collect()\n",
    "df.unpersist(blocking=True)\n",
    "rdd.unpersist(blocking=True)\n",
    "spark.catalog.clearCache()\n",
    "spark.stop()\n",
    "\n",
    "# Delete large variables if no longer needed.\n",
    "del rdd, df\n",
    "\n",
    "# Clear Jupyter Notebook Output (if applicable)\n",
    "clear_output(wait=True)\n",
    "\n",
    "print(\"Batch writing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ======================================================================\n",
    "# 4. Processing Final Output\n",
    "### ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_number(p):\n",
    "    a = array('B', p)\n",
    "    v = float(0)\n",
    "    for i in a[:-1]:\n",
    "        v = (v * 100) + ( ( (i & 0xf0) >> 4) * 10) + (i & 0xf)\n",
    "    i = a[-1]\n",
    "    v = (v * 10) + ((i & 0xf0) >> 4)\n",
    "    if (i & 0xf) == 0xd:\n",
    "        v = -v\n",
    "    return int(v)\n",
    "\n",
    "def comp_3_number(p,num):\n",
    "    a = array('B', p)\n",
    "    v = float(0)\n",
    "    for i in a[:-1]:\n",
    "        v = (v * 100) + ( ( (i & 0xf0) >> 4) * 10) + (i & 0xf)\n",
    "    i = a[-1]\n",
    "    v = (v * 10) + ((i & 0xf0) >> 4)\n",
    "    if (i & 0xf) == 0xd:\n",
    "        v = -v\n",
    "    v = v / 10**num\n",
    "    return float(v)\n",
    "\n",
    "unpack_number_udf = F.udf(unpack_number, IntegerType())\n",
    "comp_3_number_udf = F.udf(lambda p, num: comp_3_number(p, num), FloatType())\n",
    "\n",
    "# --------------------------------------------\n",
    "# Generate Column Selection using Expressions\n",
    "# --------------------------------------------\n",
    "\n",
    "def generate_column_selection(ENR_REGLT_ACC):\n",
    "    columns = []\n",
    "    for start, end, length, name, is_packed, is_comp_3 in ENR_REGLT_ACC:\n",
    "        if is_comp_3 != 0:\n",
    "            columns.append(comp_3_number_udf(F.encode(F.decode(F.substring(\"value\", start, length),charset='latin1'), charset='latin1'),F.lit(is_comp_3)).alias(name.replace('-','_')))\n",
    "        elif is_packed:\n",
    "            columns.append(unpack_number_udf(F.encode(F.substring(F.decode(\"value\", charset='latin1'), start, length), charset='latin1')).alias(name.replace('-','_')))\n",
    "        else:\n",
    "            columns.append(F.decode(F.substring(F.col(\"value\"), start, length),'latin1').alias(name.replace('-','_')))\n",
    "    return columns\n",
    "\n",
    "column_selection = generate_column_selection(sc.ENR_TFONC)\n",
    "extracted_df = df.select(*column_selection)\n",
    "extracted_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
